{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping\n",
    "![](https://i.imgur.com/6zM7JBq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the steps we'll follow:\n",
    "\n",
    "- We're going to scrape https://github.com/topics\n",
    "- We'll get a list of topics. For each topic, we'll get topic title, topic page URL and topic description\n",
    "- For each topic, we'll get the top repositories in the topic from the topic page\n",
    "- For each repository, we'll grab the repo name, username, stars and repo URL\n",
    "- For each topic we'll create a CSV file in the following format:\n",
    "\n",
    "```\n",
    "Repo Name,Username,Stars,Repo URL\n",
    "three.js,mrdoob,69700,https://github.com/mrdoob/three.js\n",
    "libgdx,libgdx,18300,https://github.com/libgdx/libgdx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have defined a lot of functions and they are for the following purpose:**\n",
    "- `get_topic_titles`, `get_topic_descs` and `get_topic_urls`  are defined to get information about all the topics\n",
    "- Then we have defined `get_topic_page`, `get_repo_info`, `get_topic_repos` to grab information of a particular repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the list of topics from Github\n",
    "Our objective in this section is to:\n",
    "\n",
    "- use requests to downlaod the page\n",
    "- user BS4 to parse and extract information\n",
    "- convert to a Pandas dataframe\n",
    "\n",
    "Let's write a function to download the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_topics_page():\n",
    "    # TODO - add comments\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_topics_page()\n",
    "# doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doc is basically html code of whole page and `get_topics_page` was defined to download the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see how inspect can be used for a line of code to directly display it's HTML code and traversing through the lines shades the area within website corresponding to the piece of code. \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "![](https://i.imgur.com/OnzIdyP.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now defining a function `get_topic_titles` to get the list of titles\n",
    "This function works as follows:\n",
    "1. We need to get the topic titles and by using inspect we observed all the titles are written under `<p>` paragraph tag having class `f3 lh-condensed mb-0 mt-1 Link--primary`.\n",
    "2. Now using find_all for above condition we will get a list(`topic_title_tags`) of all the paragraph tag containing the required information.\n",
    "3. A list naming `topic_titles` is defined in which text of all the individual tags(which we got by iterating over list `topic_title_tags`) is stored. \n",
    "4. Finally `topic_titles` is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a glimpse on the code before putting it into a function, because almost all the functions defined will have these find and find_all lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"f3 lh-condensed mb-0 mt-1 Link--primary\">3D</p>,\n",
       " <p class=\"f3 lh-condensed mb-0 mt-1 Link--primary\">Ajax</p>,\n",
       " <p class=\"f3 lh-condensed mb-0 mt-1 Link--primary\">Algorithm</p>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "topic_title_tags = doc.find_all('p', {'class': selection_class})\n",
    "topic_title_tags[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Algorithm'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_title_tags[2].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags = doc.find_all('p', {'class': selection_class})\n",
    "    topic_titles = []\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see what we have scrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D', 'Ajax', 'Algorithm', 'Amp', 'Android']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_topic_titles(doc)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly defining functions for descriptions and URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_descs(doc):\n",
    "    desc_selector = 'f5 color-fg-muted mb-0 mt-1'\n",
    "    topic_desc_tags = doc.find_all('p', {'class': desc_selector})\n",
    "    topic_descs = []\n",
    "    for tag in topic_desc_tags:\n",
    "        topic_descs.append(tag.text.strip())\n",
    "    return topic_descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D modeling is the process of virtually developing the surface and structure of a 3D object.',\n",
       " 'Ajax is a technique for creating interactive web applications.',\n",
       " 'Algorithms are self-contained sequences that carry out a variety of tasks.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_topic_descs(doc)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_urls(doc):\n",
    "    topic_link_tags = doc.find_all('a', {'class': 'no-underline flex-1 d-flex flex-column'})\n",
    "    topic_urls = []\n",
    "    base_url = 'https://github.com'\n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base_url + tag['href'])\n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above line of code tag['href'] does not contain whole url, base_url combined with tag['href'] will give us the hyperlink corresponding to the topics URL.For instance, https://github.com/topics/3d is url corresponding to 3D topic, but tag['href'] will contain only '/topics/3d' , so it is added after the base_url giving the complete URL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://github.com/topics/3d',\n",
       " 'https://github.com/topics/ajax',\n",
       " 'https://github.com/topics/algorithm']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_topic_urls(doc)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    topic_titles = get_topic_titles(doc)\n",
    "    topic_descs = get_topic_descs(doc)\n",
    "    topic_urls = get_topic_urls(doc)\n",
    "    dictionary = {'Title': topic_titles,\n",
    "                  'Description': topic_descs,\n",
    "                  'URL': topic_urls}\n",
    "    return pd.DataFrame(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3D</td>\n",
       "      <td>3D modeling is the process of virtually develo...</td>\n",
       "      <td>https://github.com/topics/3d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ajax</td>\n",
       "      <td>Ajax is a technique for creating interactive w...</td>\n",
       "      <td>https://github.com/topics/ajax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algorithm</td>\n",
       "      <td>Algorithms are self-contained sequences that c...</td>\n",
       "      <td>https://github.com/topics/algorithm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amp</td>\n",
       "      <td>Amp is a non-blocking concurrency library for ...</td>\n",
       "      <td>https://github.com/topics/amphp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Android</td>\n",
       "      <td>Android is an operating system built by Google...</td>\n",
       "      <td>https://github.com/topics/android</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Title                                        Description  \\\n",
       "0         3D  3D modeling is the process of virtually develo...   \n",
       "1       Ajax  Ajax is a technique for creating interactive w...   \n",
       "2  Algorithm  Algorithms are self-contained sequences that c...   \n",
       "3        Amp  Amp is a non-blocking concurrency library for ...   \n",
       "4    Android  Android is an operating system built by Google...   \n",
       "\n",
       "                                   URL  \n",
       "0         https://github.com/topics/3d  \n",
       "1       https://github.com/topics/ajax  \n",
       "2  https://github.com/topics/algorithm  \n",
       "3      https://github.com/topics/amphp  \n",
       "4    https://github.com/topics/android  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_df = scrape_topics()\n",
    "topics_df.loc[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping top  repositories\n",
    "Objectives of this section:\n",
    "- For a topic (say Python), I'll fetch name of top repositories there Username and  no. of star count they have\n",
    "- To acheive this various function will be defined so that those functions can be used again for the next topic\n",
    "- Then all this scrapped data will be Stored in the form of csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At first `get_topic_page` function will be defined which will retrive all the data which the corresponding topic page(Say 3D) contains using request library. \n",
    "- The response is then passed to the beautifulsoup in the form of text which then returns an object, which you can use for further searches or to extract its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    # Download the page\n",
    "    response = requests.get(topic_url)\n",
    "    # Check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    # Parse using Beautiful soup\n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_topic_page('https://github.com/topics/3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc # Uncomment to see what it contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After Inspecting the various parts of the website, what i observed is the article tag contains all the necessary information regarding repo title, repo URL and username.**\n",
    "Now a function `get_repo_info()` is defined which will return username, repo_name, stars and repo_url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://github.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(article_tag, star_tag):\n",
    "    # returns all the required info about a repository\n",
    "    a_tags = article_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url =  base_url + a_tags[1]['href']\n",
    "    stars = star_tag.text.strip()\n",
    "    return username, repo_name, stars, repo_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example to display this functions takes as input and what does it returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span aria-label=\"83599 users starred this repository\" class=\"Counter js-social-count\" data-pjax-replace=\"true\" data-plural-suffix=\"users starred this repository\" data-singular-suffix=\"user starred this repository\" data-turbo-replace=\"true\" data-view-component=\"true\" id=\"repo-stars-counter-star\" title=\"83,599\">83.6k</span>\n"
     ]
    }
   ],
   "source": [
    "# Get the article tags containing repo title, repo URL and username\n",
    "repo_tags = doc.find_all('article',{'class':'border rounded color-shadow-small color-bg-subtle my-4'})\n",
    "a_tag = repo_tags[0].find_all('a') \n",
    "\n",
    "# Get star tags\n",
    "star_tags=doc.find_all('span',{'id':'repo-stars-counter-star'})\n",
    "# print(a_tag[0], \"\\n\") # Uncomment to see what it contains\n",
    "print(star_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', 'mrdoob', '83.6k', 'https://github.com/mrdoob')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_repo_info(repo_tags[0], star_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we will define `get_topic_repos()` function which will take doc(returned by `get_topic_page()` function) and then using `get_repo_info()` it will fetch data.\n",
    "- This data will be converted into dictionary `topic_repos_dict` which will be finally converted to pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_doc):\n",
    "    # Get the article tags containing repo title, repo URL and username\n",
    "    repo_tags = topic_doc.find_all('article',{'class':'border rounded color-shadow-small color-bg-subtle my-4'})\n",
    "\n",
    "    # Get star tags\n",
    "    star_tags=topic_doc.find_all('span',{'id':'repo-stars-counter-star'})\n",
    "    \n",
    "    topic_repos_dict = { 'username': [], 'repo_name': [], 'stars': [],'repo_url': []}\n",
    "\n",
    "    # Get repo info\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
    "        \n",
    "    return pd.DataFrame(topic_repos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://github.com/topics/3d',\n",
       " 'https://github.com/topics/ajax',\n",
       " 'https://github.com/topics/algorithm']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_urls = get_topic_urls(get_topics_page())\n",
    "topic_urls[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>stars</th>\n",
       "      <th>repo_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>jwasham</td>\n",
       "      <td>225k</td>\n",
       "      <td>https://github.com/jwasham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CyC2018</td>\n",
       "      <td>CS-Notes</td>\n",
       "      <td>154k</td>\n",
       "      <td>https://github.com/CyC2018/CS-Notes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>trekhleb</td>\n",
       "      <td>146k</td>\n",
       "      <td>https://github.com/trekhleb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TheAlgorithms</td>\n",
       "      <td>Python</td>\n",
       "      <td>140k</td>\n",
       "      <td>https://github.com/TheAlgorithms/Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yangshun</td>\n",
       "      <td>tech-interview-handbook</td>\n",
       "      <td>74.4k</td>\n",
       "      <td>https://github.com/yangshun/tech-interview-han...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        username                repo_name  stars  \\\n",
       "0                                 jwasham   225k   \n",
       "1        CyC2018                 CS-Notes   154k   \n",
       "2                                trekhleb   146k   \n",
       "3  TheAlgorithms                   Python   140k   \n",
       "4       yangshun  tech-interview-handbook  74.4k   \n",
       "\n",
       "                                            repo_url  \n",
       "0                         https://github.com/jwasham  \n",
       "1                https://github.com/CyC2018/CS-Notes  \n",
       "2                        https://github.com/trekhleb  \n",
       "3            https://github.com/TheAlgorithms/Python  \n",
       "4  https://github.com/yangshun/tech-interview-han...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_topic_repos(get_topic_page(topic_urls[2])).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a function to convert the dataframe returned by `get_topic_repos` into a csv file in the following format:\n",
    "\n",
    "```\n",
    "Repo Name,Username,Stars,Repo URL\n",
    "three.js,mrdoob,69700,https://github.com/mrdoob/three.js\n",
    "libgdx,libgdx,18300,https://github.com/libgdx/libgdx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def csv_converter(topic_url, path):\n",
    "    if os.path.exists(path):\n",
    "        print(\"The file {} already exists...\".format(path))\n",
    "        return\n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    topic_df.to_csv(path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file C:\\Users\\HP\\jupytr notebook\\Web Scrapping\\Chrome extension.csv already exists...\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()+\"\\\\\"+get_topic_titles(get_topics_page())[21]+\".csv\"\n",
    "csv_converter(topic_urls[21], path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's read above csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        username                 repo_name  stars                                                  repo_url\n",
      "0     iamadamdev    bypass-paywalls-chrome  26.1k      https://github.com/iamadamdev/bypass-paywalls-chrome\n",
      "1            NaN                jaywcjlove  18.7k                             https://github.com/jaywcjlove\n",
      "2            NaN            refined-github  17.9k                         https://github.com/refined-github\n",
      "3            NaN                   checkly  14.2k                                https://github.com/checkly\n",
      "4            NaN                darkreader    14k                             https://github.com/darkreader\n",
      "5          unbug                    codelf  12.9k                           https://github.com/unbug/codelf\n",
      "6        Anarios    return-youtube-dislike   9.4k         https://github.com/Anarios/return-youtube-dislike\n",
      "7            NaN                     crimx   9.2k                                  https://github.com/crimx\n",
      "8            NaN            gildas-lormeau   8.5k                         https://github.com/gildas-lormeau\n",
      "9        listen1  listen1_chrome_extension   8.5k       https://github.com/listen1/listen1_chrome_extension\n",
      "10     appbaseio                    dejavu   7.8k                       https://github.com/appbaseio/dejavu\n",
      "11           NaN                 alyssaxuu   7.2k                              https://github.com/alyssaxuu\n",
      "12           NaN               dailydotdev   7.1k                            https://github.com/dailydotdev\n",
      "13           NaN                   Kenshin   5.9k                                https://github.com/Kenshin\n",
      "14        ajayyy              SponsorBlock   4.9k                    https://github.com/ajayyy/SponsorBlock\n",
      "15           NaN                manojVivek   4.8k                             https://github.com/manojVivek\n",
      "16           NaN                   dessant   4.8k                                https://github.com/dessant\n",
      "17     AutomaApp                    automa   4.6k                       https://github.com/AutomaApp/automa\n",
      "18           NaN                 alyssaxuu   4.6k                              https://github.com/alyssaxuu\n",
      "19           NaN            altair-graphql   4.2k                         https://github.com/altair-graphql\n",
      "20    webclipper               web-clipper   4.2k                 https://github.com/webclipper/web-clipper\n",
      "21         zxlie                  FeHelper   4.1k                         https://github.com/zxlie/FeHelper\n",
      "22    openstyles                    stylus   3.9k                      https://github.com/openstyles/stylus\n",
      "23           NaN           bilibili-helper   3.8k                        https://github.com/bilibili-helper\n",
      "24         dejan               rails_panel   3.7k                      https://github.com/dejan/rails_panel\n",
      "25       hcfyapp   crx-selection-translate   3.7k        https://github.com/hcfyapp/crx-selection-translate\n",
      "26  KartikTalwar                  gmail.js   3.5k                  https://github.com/KartikTalwar/gmail.js\n",
      "27  honestbleeps  Reddit-Enhancement-Suite   3.5k  https://github.com/honestbleeps/Reddit-Enhancement-Suite\n",
      "28    WorldBrain                     Memex   3.5k                       https://github.com/WorldBrain/Memex\n",
      "29      ronggang            PT-Plugin-Plus   3.4k                https://github.com/ronggang/PT-Plugin-Plus\n"
     ]
    }
   ],
   "source": [
    "csv_file = pd.read_csv(path)\n",
    "print(csv_file.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, Putting it all together\n",
    "\n",
    "- We have a funciton to get the list of topics\n",
    "- We have a function to create a CSV file for scraped repos from a topics page\n",
    "- Let's create a function to put them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3D</td>\n",
       "      <td>3D modeling is the process of virtually develo...</td>\n",
       "      <td>https://github.com/topics/3d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ajax</td>\n",
       "      <td>Ajax is a technique for creating interactive w...</td>\n",
       "      <td>https://github.com/topics/ajax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algorithm</td>\n",
       "      <td>Algorithms are self-contained sequences that c...</td>\n",
       "      <td>https://github.com/topics/algorithm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amp</td>\n",
       "      <td>Amp is a non-blocking concurrency library for ...</td>\n",
       "      <td>https://github.com/topics/amphp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Android</td>\n",
       "      <td>Android is an operating system built by Google...</td>\n",
       "      <td>https://github.com/topics/android</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Title                                        Description  \\\n",
       "0         3D  3D modeling is the process of virtually develo...   \n",
       "1       Ajax  Ajax is a technique for creating interactive w...   \n",
       "2  Algorithm  Algorithms are self-contained sequences that c...   \n",
       "3        Amp  Amp is a non-blocking concurrency library for ...   \n",
       "4    Android  Android is an operating system built by Google...   \n",
       "\n",
       "                                   URL  \n",
       "0         https://github.com/topics/3d  \n",
       "1       https://github.com/topics/ajax  \n",
       "2  https://github.com/topics/algorithm  \n",
       "3      https://github.com/topics/amphp  \n",
       "4    https://github.com/topics/android  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_df.head() # This has been defined earlier as well and this is same as used in function below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is working as follows:\n",
    "1. Using function scrape_topics() it is taking the pandas dataframe shown above.\n",
    "2. Then a directory naming data has been created using the os module \n",
    "3. In the data directory all the csv files made through csv_convertor has been stored\n",
    "4. To iterate over the rows of dataframe iterrows() has been used, this divides whole dataframe into individual rows and a row contains the index along with the list which can be sliced to get tequired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic_repos():\n",
    "    print('Scraping list of topics')\n",
    "    topics_df = scrape_topics() # See topics_df above for better understanding\n",
    "    \n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    for index, row in topics_df.iterrows(): # looping over rows in pandas dataframe\n",
    "        print('Scraping top repositories for \"{}\"'.format(row['Title']))\n",
    "        csv_converter(row['URL'], 'data/{}.csv'.format(row['Title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's the magic of web scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics\n",
      "Scraping top repositories for \"3D\"\n",
      "Scraping top repositories for \"Ajax\"\n",
      "Scraping top repositories for \"Algorithm\"\n",
      "Scraping top repositories for \"Amp\"\n",
      "Scraping top repositories for \"Android\"\n",
      "Scraping top repositories for \"Angular\"\n",
      "Scraping top repositories for \"Ansible\"\n",
      "Scraping top repositories for \"API\"\n",
      "Scraping top repositories for \"Arduino\"\n",
      "Scraping top repositories for \"ASP.NET\"\n",
      "Scraping top repositories for \"Atom\"\n",
      "Scraping top repositories for \"Awesome Lists\"\n",
      "Scraping top repositories for \"Amazon Web Services\"\n",
      "Scraping top repositories for \"Azure\"\n",
      "Scraping top repositories for \"Babel\"\n",
      "Scraping top repositories for \"Bash\"\n",
      "Scraping top repositories for \"Bitcoin\"\n",
      "Scraping top repositories for \"Bootstrap\"\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Failed to load page https://github.com/topics/bootstrap",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-eb2908dc3bee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscrape_topic_repos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-1277b6684c56>\u001b[0m in \u001b[0;36mscrape_topic_repos\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopics_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# looping over rows in pandas dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Scraping top repositories for \"{}\"'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mcsv_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'URL'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data/{}.csv'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-9b8cc250e862>\u001b[0m in \u001b[0;36mcsv_converter\u001b[1;34m(topic_url, path)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The file {} already exists...\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtopic_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_topic_repos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_topic_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtopic_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-00314fca937d>\u001b[0m in \u001b[0;36mget_topic_page\u001b[1;34m(topic_url)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# Check successful response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Failed to load page {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Parse using Beautiful soup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtopic_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Failed to load page https://github.com/topics/bootstrap"
     ]
    }
   ],
   "source": [
    "scrape_topic_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the data directory in which all the csv files have been stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.listdir('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now reading 3D.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv_file = pd.read_csv('data\\\\3D.csv')\n",
    "print(csv_file.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
